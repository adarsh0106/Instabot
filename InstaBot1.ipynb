{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstaBot Project - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)Login to your Instagram Handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1)Submit with sample username and password "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys                            #provide keys in the keyboard like RETURN, F1, ALT\n",
    "from selenium.webdriver.support.ui import Select                           #to select the option html attribute\n",
    "from selenium.webdriver.support.ui import WebDriverWait                    #To use implcit and explicit wait\n",
    "from selenium.webdriver.support import expected_conditions as EC           #use in explicitly wait\n",
    "from selenium.webdriver.common.by import By                                #to select the attribute by Class,link_text\n",
    "from selenium import webdriver                                             #import web Driver\n",
    "import time                                                                #it use in wait\n",
    "from bs4 import BeautifulSoup                                              #work with attribute \n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path = 'C:/chromedriver')             #invoke chrome webdriver\n",
    "driver.get('https://www.instagram.com/')                                        #passing the instagram website link\n",
    "wait = WebDriverWait(driver, 10)                                                #wait to 10ms\n",
    "login_btn = wait.until(EC.presence_of_element_located((By.LINK_TEXT,'Log in'))) #fetching login button\n",
    "login_btn.click()                                                               #click on login button\n",
    "time.sleep(1)                                                                   #1sec sleep to move next page\n",
    "d = wait.until(EC.presence_of_element_located((By.NAME,'username')))            #click on username input box\n",
    "d.send_keys('SAMPLE USERNAME')                                                         #sedning user name\n",
    "e=driver.find_element_by_name('password')                                       #click on password box\n",
    "e.send_keys('SAMPLE PASSWORD')                                                        #send password\n",
    "login=driver.find_element_by_class_name('L3NKy')                                #serch for login button\n",
    "login.click()                                                                   #click on login button\n",
    "exit_btn = wait.until(EC.presence_of_element_located((By.XPATH,'//div/button[contains(@class,\"HoLwm\")]'))) #search exit button \n",
    "exit_btn.click()                                                                #exit button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)Type for “food” in search bar and print all the names of the Instagram Handles that are displayed in list after typing “food”\n",
    "\n",
    "### Note : Make sure to avoid printing hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food.marine\n",
      "foodmaniacindia\n",
      "foodrush.recipe\n",
      "foodie_incarnate\n",
      "foodnetwork\n",
      "foodhallindia\n",
      "food\n",
      "foodytops\n",
      "foodtalkindia\n",
      "foodiesince96\n",
      "food_connects_the_world\n",
      "yourfoodlab\n",
      "dilsefoodie\n",
      "foodnetworkkitchen\n",
      "foodchoiceofmumbai\n",
      "foodofmumbai\n",
      "fooduz\n",
      "foodie_rushil\n",
      "foodandwine\n",
      "food52\n",
      "foodhallcookerystudio\n",
      "recipebae\n",
      "hangouts.co.in\n",
      "foodiecouple_parvan\n",
      "usefuldiyss\n",
      "mumbaifoodie\n",
      "buzzfeedfood\n",
      "selectfoody\n",
      "food.darzee\n",
      "foodofchefs\n",
      "luxurious_tarunwadhwani\n",
      "cchannel_food\n"
     ]
    }
   ],
   "source": [
    "a=driver.find_element_by_xpath('//input[contains(@class,\"XTCLo\")]')                    #serch box for passing input\n",
    "a.send_keys('food')                                                                    #passing food\n",
    "time.sleep(3)                                                                          #sleep 3 sec.\n",
    "handles = driver.find_elements_by_xpath('//div[@class = \"fuqBx\"]/a[\"href\"]')           #fecting top food list hadles \n",
    "food_list = []   \n",
    "for i in handles:\n",
    "    if 'explore' in i.get_attribute('href'):                                    #if explore present in link then it is hastags\n",
    "        continue\n",
    "    else:       \n",
    "        s = i.get_attribute('href').split('/')                                  #https://www.instagram.com/foodtalkindia\n",
    "        print(s[3])                                     # after split s= ['https:','','www.instagram.com','foodtalkindia']\n",
    "        food_list.append(s[3])                          #appending s[3] in food_list       \n",
    "a=driver.find_element_by_xpath('//input[contains(@class,\"XTCLo\")]') #serac input box\n",
    "a.clear()                                                           #clear box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_element_by_class_name('coreSpriteSearchClear') #selecting clear box by class name\n",
    "a.click()                                                    #click on clear box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3)Searching and Opening a profile using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open profile of “So Delhi”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_element_by_xpath('//input[contains(@class,\"XTCLo\")]')         #input box     \n",
    "a.send_keys('sodelhi')\n",
    "b = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'yCE8d')))     #selecting sodelhi\n",
    "b.click()\n",
    "a=driver.find_element_by_class_name('coreSpriteSearchClear') #selecting clear box by class name\n",
    "a.click()                                                    #click on clear box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)Follow/Unfollow given handle -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Open the Instagram Handle of “So Delhi”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_element_by_xpath('//input[contains(@class,\"XTCLo\")]')  #input box\n",
    "a.send_keys('sodelhi')\n",
    "b = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'yCE8d'))) #serarch by class name\n",
    "b.click()\n",
    "a=driver.find_element_by_class_name('coreSpriteSearchClear') #selecting clear box by class name\n",
    "a.click()                                                    #click on clear box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) Start following it. Print a message if you are already following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Following The Account\n"
     ]
    }
   ],
   "source": [
    "follow_btn = wait.until(EC.presence_of_element_located((By.XPATH,'//span[contains(@class,\"vBF20\")]/button')))  #for follow button\n",
    "if follow_btn.text.strip() == 'Following':      #checking the status\n",
    "    print(\"Already Following The Account\")\n",
    "else:\n",
    "    follow_btn.click()                          #if not following click on the follow button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.3) After following, unfollow the instagram handle. Print a message if you have already unfollowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if follow_btn.text.strip() == 'Following':   #chcking status\n",
    "    follow_btn.click()                       # clicking on follow button, but already following so ask for unfollow\n",
    "    Unfollow_btn = driver.find_element_by_xpath('//div[contains(@class,\"mt3GC\")]/button') #for unfollow button\n",
    "    Unfollow_btn.click()                     #unfollow button\n",
    "else:\n",
    "    print(\"Already UNFollowed The Account\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.1) Liking the top 30 posts of the ‘dilsefoodie'. Print message if you have already liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Liked No. 1 Post\n",
      "Already Liked No. 2 Post\n",
      "Already Liked No. 3 Post\n",
      "Already Liked No. 4 Post\n",
      "Already Liked No. 5 Post\n",
      "Already Liked No. 6 Post\n",
      "Already Liked No. 7 Post\n",
      "Already Liked No. 8 Post\n",
      "Already Liked No. 9 Post\n",
      "Already Liked No. 10 Post\n",
      "Already Liked No. 11 Post\n",
      "Already Liked No. 12 Post\n",
      "Already Liked No. 13 Post\n",
      "Already Liked No. 14 Post\n",
      "Already Liked No. 15 Post\n",
      "Already Liked No. 16 Post\n",
      "Already Liked No. 17 Post\n",
      "Already Liked No. 18 Post\n",
      "Already Liked No. 19 Post\n",
      "Already Liked No. 20 Post\n",
      "Already Liked No. 21 Post\n",
      "Already Liked No. 22 Post\n",
      "Already Liked No. 23 Post\n",
      "Already Liked No. 24 Post\n",
      "Already Liked No. 25 Post\n",
      "Already Liked No. 26 Post\n",
      "Already Liked No. 27 Post\n",
      "Already Liked No. 28 Post\n",
      "Already Liked No. 29 Post\n",
      "Already Liked No. 30 Post\n"
     ]
    }
   ],
   "source": [
    "search = driver.find_element_by_xpath('//div[contains(@class,\"LWmhU\")]/input')     #input box\n",
    "search.send_keys('dilsefoodie')                                                    #sending keys\n",
    "b = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'z556c')))            #searching sodelhi profile  \n",
    "b.click()                                                                          #clicking on profile\n",
    "for i in range(3):\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')        #infirst scroll we get 12 post so we making 3\n",
    "    time.sleep(2)                                     \n",
    "post = driver.find_elements_by_xpath('//div[contains(@class,\"v1Nh3\")]')            #finding post \n",
    "count = 1\n",
    "for i in post:\n",
    "    if count == 31:                                                                #checking for only 30 post \n",
    "        break\n",
    "        \n",
    "    i.click()                                                                      #clicking on each post\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    like_btn = wait.until(EC.presence_of_element_located((By.XPATH,'//span[contains(@class,\"fr66n\")]/button/span')))#for like button\n",
    "    if like_btn.get_attribute('aria-label') == 'Unlike':                           #checking status\n",
    "        print(\"Already Liked No.\",count,\"Post\")\n",
    "    else:\n",
    "        like_btn.click()                                                           #clicking on like button\n",
    "    close_btn = driver.find_element_by_class_name('ckWGn')                         # post close button\n",
    "    close_btn.click()                                                              #click on close utton\n",
    "    time.sleep(1)\n",
    "    count += 1                                                                     #counting post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.2) Unliking the top 30 posts of the ‘dilsefoodie’. Print message if you have already unliked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for i in post:\n",
    "    if count == 31:                             #only for 30 post codition\n",
    "        break\n",
    "        \n",
    "    i.click()                                   #clicking on post\n",
    "    time.sleep(1)\n",
    "    unlike_btn = wait.until(EC.presence_of_element_located((By.XPATH,'//span[contains(@class,\"fr66n\")]/button/span'))) #for like button\n",
    "    if unlike_btn.get_attribute('aria-label') == 'Like':  #checking status\n",
    "        print(\"Already UnLiked No.\",count,\"Post\")\n",
    "    else:\n",
    "        unlike_btn.click()     #clicking unlike button\n",
    "    close_btn = driver.find_element_by_class_name('ckWGn')           #post close buttonn\n",
    "    close_btn.click()                                                #clicking on close button \n",
    "    time.sleep(1)\n",
    "    count += 1                                                       #counting post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6.1) Extract the usernames of the first 500 followers of ‘foodtalkindia’ and ‘sodelhi’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\public\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\public\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ankur._.k_\n",
      "glomadicmusafir\n",
      "vctory10\n",
      "___attitude__lover___\n",
      "jerrin.p_george\n",
      "__king__of__attitude\n",
      "nikhilkichcha\n",
      "its.kr.shivam\n",
      "_.ginni7._\n",
      "tejesh_chowdary77\n",
      "mirzanajmussakib\n",
      "sasteyfoodies\n",
      "_pjskitchen_\n",
      "21ha__b\n",
      "immaturecartoon5297\n",
      "usham_sidana_\n",
      "prience.tushar\n",
      "simar_wadhwa\n",
      "saloniahuja\n",
      "yaseminnn.t\n",
      "mj__nirauu_46\n",
      "kirupatil_2102\n",
      "priyanka.jogdand.790\n",
      "anildeal525\n",
      "aashna.pradhan20\n",
      "bhavna_srivas\n",
      "itsme_nagori\n",
      "_fooddiess_\n",
      "_dish_a_day\n",
      "rasaaldwivedi\n",
      "manmeet.kinny\n",
      "shubhs75\n",
      "laxmansingh8027\n",
      "fnlmtbkhlrby\n",
      "mirzoev2oo2\n",
      "zohaib.maqsood143\n",
      "arshie_pvt\n",
      "__ng_56\n",
      "the_opendoors\n",
      "tanmaysharma.2\n",
      "w_t_e_v_r\n",
      "v_j_vekariya_7699\n",
      "clueless_dumbo\n",
      "imi.meenu\n",
      "saritasorathia\n",
      "jeet_deshwal\n",
      "nishant8211\n",
      "indianbackpackersstory\n",
      "foodiehutfusion\n",
      "hoppibeerapp\n",
      "just_quotes_thing\n",
      "vikrampatel540\n",
      "sana_19849\n",
      "ishita.mangla\n",
      "ash_21_96\n",
      "pawanpilaniya1606\n",
      "_jeffy_chen\n",
      "therealfoodzone26\n",
      "____theastralexplorer____\n",
      "its_adi._12999\n",
      "nameisnotatopic\n",
      "y_kajal_kj\n",
      "mohantyagi_official\n",
      "momomiyaan\n",
      "ver.francis\n",
      "kkajall_\n",
      "emran0606\n",
      "kashysp_raj\n",
      "karuna9949\n",
      "x_ansu07\n",
      "anuradhajampana\n",
      "boby_sid_786\n",
      "vaidehisharmamusic\n",
      "bashameed2503\n",
      "swadmaharashtracha\n",
      "thefoodiesista\n",
      "archana.j14\n",
      "shanusharma270859\n",
      "sunnyjaryal7\n",
      "subodh.sudhanshu24\n",
      "raj_thakkar2\n",
      "brainoutscience0\n",
      "fan_of_official\n",
      "shreya_._19\n",
      "miss_k.d.b\n",
      "_aafat_1027\n",
      "sajid.pacha\n",
      "mr_._.sohail.01\n",
      "sourav.jana.75641\n",
      "srkhappy66\n",
      "krishaparekh_\n",
      "chefandreamura\n",
      "raj.rahul85482019\n",
      "sunilkumar20181818\n",
      "carla.joven\n",
      "yuvika_jain20\n",
      "hparmar90\n",
      "charles_wild_life\n",
      "mr__sandip_\n",
      "2thefood\n",
      "_medico_2001\n",
      "rishika_singh007\n",
      "pili9142\n",
      "milind_palany\n",
      "t.samruddhi_5\n",
      "prayanggarg18\n",
      "khalsahoteldhaba\n",
      "ne_hahaha._\n",
      "gaurav_khaitan\n",
      "days_are_out\n",
      "abdalsalam810\n",
      "pradhankunjan\n",
      "rameshbishoi\n",
      "sanket_guitar\n",
      "anishbhatti333\n",
      "ishaansbottlechuska\n",
      "indian_poems\n",
      "shiva2811\n",
      "samod__spices\n",
      "its_me_oberoi\n",
      "yash_._02\n",
      "2xmile_india\n",
      "jethaniba\n",
      "ashta_ash\n",
      "ooly_92_\n",
      "henna_by_rinks\n",
      "hind0500\n",
      "ankitd_555\n",
      "busy.eyes\n",
      "saurabhyadav7099\n",
      "moksha_jain28\n",
      "icimranmatharoo\n",
      "paluck28\n",
      "shubham_18_07\n",
      "nik19.20\n",
      "gnanasoundari_17\n",
      "sappyfeed_7\n",
      "aliya_khan1152\n",
      "gourmet_desi\n",
      "shivathakorshivathakor15\n",
      "adalee7711\n",
      "viamilanorestaurants\n",
      "samiksha53781\n",
      "_jbraa._srk__fighter_056\n",
      "stubborn_manyari\n",
      "adhavt\n",
      "monishakishore\n",
      "foodie_hub_ranchi\n",
      "shrepps\n",
      "serve_justice_1\n",
      "tandoorinites\n",
      "mshraddha707\n",
      "never_stop_exploring___\n",
      "kssk1514\n",
      "khannariddhima\n",
      "ishaan9359\n",
      "theyogiboxer\n",
      "raftaar_sheikh_786\n",
      "foodholic_weight_\n",
      "tarun.chahal\n",
      "abhi.cold_flame\n",
      "suryagugale\n",
      "_monochromatic.hearted\n",
      "kp274nv\n",
      "yadavharsh1970\n",
      "the_phytophage\n",
      "diners.kaleidoscope\n",
      "official_pratu_\n",
      "vrinda.x\n",
      "bibin_cc\n",
      "__aaaa.u.sheeee__\n",
      "kavitha_pgk\n",
      "tayabashabir\n",
      "aakrutidedhia\n",
      "_ishteyaque_alam\n",
      "aliafsar016\n",
      "stone.the.builder.refused\n",
      "raag_a_foodtale\n",
      "shubhambag_007\n",
      "_snowy_secret_42_\n",
      "abhishek_kapoor\n",
      "ervisfoundation\n",
      "raks_721\n",
      "miracles_divya\n",
      "mr_sagacasm\n",
      "ghsan9616\n",
      "mukulkrsingh\n",
      "hungrykart_designer\n",
      "alibideshki7534\n",
      "supriyabhoj\n",
      "ashis.dash.52012\n",
      "animasinha72gmail.com_\n",
      "made._in._charms\n",
      "balpreetsingh5858\n",
      "ria.dey_10\n",
      "katariyasajjan\n",
      "prakhrgupta\n",
      "foody_gupta_girl\n",
      "karunansharma\n",
      "pranavgupta_27\n",
      "kesharwani6781\n",
      "meenalshejwar\n",
      "mukkumanvani\n",
      "ruchi21in\n",
      "all_about_fooood_\n",
      "bhavya_ranavat_8672\n",
      "asrorova.nilufar\n",
      "shrutiaggarwal_espanola\n",
      "_ps_private\n",
      "sanjeevbanga\n",
      "kelechidimm\n",
      "arjundhamanda\n",
      "adithi___29\n",
      "craveable_flavours_food\n",
      "vedikathukral\n",
      "geetikabhat\n",
      "fiery_miracle\n",
      "dicksonpebam\n",
      "palak_234\n",
      "eatbyhand_ebh\n",
      "crazy__boy_345\n",
      "_sharayu_17\n",
      "saritaperiwal56\n",
      "_nostalgia_no_limits_\n",
      "meghaajhaa\n",
      "world_for_food\n",
      "soumi585\n",
      "pankajbahl08\n",
      "rajesh.salveru\n",
      "aamhikhandeshi.resto\n",
      "nautiyalharshu\n",
      "mirzabilal25\n",
      "chooljaipur\n",
      "_anamika_roy12_\n",
      "amyloony786\n",
      "i.m.bliss\n",
      "_abhishek_19_\n",
      "food_bytee\n",
      "viraj12_5\n",
      "gurmeetkaalra\n",
      "siddiq661\n",
      "anubhavsharma_tsboh\n",
      "cafediaryindia\n",
      "passionofflavours8\n",
      "amo1175\n",
      "vishalsethijii\n",
      "na.vya9012\n",
      "tastosaurous\n",
      "theincomparable1\n",
      "prathu__44\n",
      "naveen.5276\n",
      "rahulrana5104\n",
      "sanashaikh9678\n",
      "gorakhpurfooding_up53\n",
      "meet_the_meet_987\n",
      "p_r_s__g_pl_n_\n",
      "ashutasomkuwar\n",
      "anjaliroy7362\n",
      "deepalikarkare\n",
      "bhandari_appu\n",
      "ahmadhasim\n",
      "foodie2808\n",
      "ganesh_balpande\n",
      "zahedasultana207\n",
      "robertinhatabatchnik\n",
      "yulin.esm\n",
      "megha_salecha\n",
      "thatdaftgirl\n",
      "aggankita\n",
      "sanghasree_snowy\n",
      "fooodiiiee_\n",
      "zaidshaikh200070\n",
      "shireensummy1_\n",
      "yashmisingh\n",
      "agnidipa\n",
      "arkcafe_\n",
      "soniatwal04\n",
      "faylovie94\n",
      "bartonfresh\n",
      "wowz_food\n",
      "ninaaarohi\n",
      "dhirajmane4161\n",
      "mithalisamson\n",
      "chitrarung\n",
      "sahil_nandanwar_1\n",
      "sonalisahu144\n",
      "shashikanth27\n",
      "foodieecrush\n",
      "stayhealthylivelong\n",
      "chandardeepparota\n",
      "foodpagli\n",
      "jiteshcc29\n",
      "chandankumarbarh034\n",
      "itsallabout_maneetkaur\n",
      "bhavya_dewan\n",
      "s_b_collection__\n",
      "dozensofstoriesmumbai\n",
      "oo7.leo\n",
      "poonamrajput5226\n",
      "the.bookcafe\n",
      "k_u_n_w_a_r___s_a_a\n",
      "drspsb\n",
      "shaikh_rasheeda\n",
      "samgawade\n",
      "tarun_gupta532\n",
      "lizalizu3\n",
      "gaurav.dodo\n",
      "rhythm_winery\n",
      "leo_ankan\n",
      "ramarao.ramarao.961993\n",
      "_shreyashree._\n",
      "ammy_mishra\n",
      "suruchibhasin03\n",
      "ishita_tintin\n",
      "skylarktourstravels\n",
      "alyuko9\n",
      "4658.kevin\n",
      "amarsingh__rajput\n",
      "rishianeja\n",
      "yashguptaji\n",
      "bioxpradheep\n",
      "toto_little_rabbit\n",
      "ma.njeet6002\n",
      "dattwalswapnil\n",
      "nayabalamnew\n",
      "bhabhi.girl_\n",
      "anupamadanika\n",
      "berghemfood\n",
      "_manoj_g22\n",
      "indori_namkeens\n",
      "sukanyagaikwad.p22\n",
      "kangscookingcorner\n",
      "singhprathamrajput_\n",
      "taste.of_love\n",
      "chavdahasti01\n",
      "patole_aishwarya\n",
      "luckynayna\n",
      "mmohsinraj\n",
      "ruman_026\n",
      "vaghelavaghela5555\n",
      "kiran_kalleti\n",
      "daminira2\n",
      "laddubafla\n",
      "snsd_foodspot\n",
      "pooja.jaikar\n",
      "iamlekhaperugu259\n",
      "insanemahi2610\n",
      "manasi_vt\n",
      "hemank3032\n",
      "frzi______\n",
      "praveshshandilya\n",
      "khusbu_kejriwal\n",
      "parkprimejaipur\n",
      "digitalworkshop77\n",
      "sharma.user\n",
      "ridhigargmangla\n",
      "vinesastro\n",
      "yard_of_junkies\n",
      "vera.cruz9\n",
      "dissshhhhh\n",
      "deepsd\n",
      "n_o_t_h_i_n_g._.s_p_e_c_i_a_l\n",
      "thecarbdashian1.0\n",
      "ayubowantlk\n",
      "a.ali.sunasara\n",
      "riddhichuriwal\n",
      "amitsingh70007\n",
      "yummmlok\n",
      "b_e_i_n_g__e_x_i_m_i_o_u_s_30_\n",
      "whatsonmyplatee\n",
      "mr_samuel_13\n",
      "madamruchie\n",
      "sandeepsinghrathore5443\n",
      "bucketlist.vacations\n",
      "rikubarman2021\n",
      "sulabh71\n",
      "sunnys_culinary\n",
      "harshitharevanna\n",
      "girl_with_latent_tiara\n",
      "tanimamondal\n",
      "bakestudio_ajmer\n",
      "indianfoodiestories\n",
      "setu29\n",
      "indisposable_soul\n",
      "bhukkad_adda\n",
      "rahulyadav.57\n",
      "whatdfoodies\n",
      "eqraesthetic\n",
      "jannijanuu\n",
      "surbhit19\n",
      "the_outpostwriter1234\n",
      "dipali7421\n",
      "devyani_agrawal\n",
      "jyotirolla\n",
      "aqtalpur\n",
      "aman_vagadiya12\n",
      "j_k__das\n",
      "anjaniz.foodelicious\n",
      "olive_stanley8691\n",
      "sanwager_kitchen\n",
      "shubham.choudhary1310\n",
      "simmioberai\n",
      "jatinderlekhi\n",
      "preekama\n",
      "candice0597\n",
      "ghoshbagchi\n",
      "pagareshraddha1911\n",
      "ipsitabi\n",
      "call_me_singh1\n",
      "kriti__1506\n",
      "wardrobebysonu\n",
      "ruchikaaroradesign\n",
      "poweredby_caffeine\n",
      "indhu_yalla\n",
      "quee.nie692\n",
      "neelu0710\n",
      "om_baymax\n",
      "vishwakarma.ar\n",
      "sarita_das1507\n",
      "aneeshbhuddi\n",
      "padmajafamilyrestoandbar\n",
      "travelholicayesha\n",
      "chitra_sanjith\n",
      "paruldilip\n",
      "kishan_kachhadiya\n",
      "omkar712\n",
      "prem_ayama\n",
      "gabru_spirit\n",
      "kritak_sketch\n",
      "ikirtiagja\n",
      "riditikkk\n",
      "khushdeep50\n",
      "youare_chef\n",
      "rit_vik_77\n",
      "sicilian_beyond_pizza\n",
      "winchesterbrothers_ll\n",
      "mr.rudransh\n",
      "wandering_swans\n",
      "sunaina6469\n",
      "swarnaliclicks\n",
      "sherusiyota\n",
      "muhammedsawad15\n",
      "ujjwaldas554\n",
      "mayur5260\n",
      "eating_blogging_eating\n",
      "vatsaalpana\n",
      "swag_bebe_nishtha\n",
      "ashishmthakor\n",
      "pranjalporwal8\n",
      "sharmashubhammm\n",
      "jaisalmergin\n",
      "hotel_aagaaz\n",
      "common_senser\n",
      "khane_ki_shokeen\n",
      "sonalibahuguna2019\n",
      "enwegoglobal\n",
      "akansha24dixit\n",
      "bhakti.24\n",
      "k.naik92\n",
      "jasdeepsingh_malhotra\n",
      "mit_chell_m2\n",
      "vkondil\n",
      "priyanka_sathish\n",
      "queensellfie\n",
      "saagarlakde\n",
      "solankiupen\n",
      "dhwanisawhney\n",
      "kirtinayak26\n",
      "i_arunkumar1808\n",
      "aao.bhojan.kare\n",
      "vermavinod2110\n",
      "neelesh1394\n",
      "nehayadav82182019\n",
      "showmik2407\n",
      "_prarthana.mohan_\n",
      "theindiansalt\n",
      "photopado\n",
      "gurdhian_sandhu_0999\n",
      "cleveshit\n",
      "journey_of_eatingfood\n",
      "its_official_thind_\n",
      "starthawke_mystery\n",
      "daddyzlittle_gal\n",
      "shwetaa_kanojiya\n",
      "rakhidua_81\n",
      "sak_she\n",
      "rampursinglemalt\n",
      "lav2603\n",
      "pavan_ni_moj\n",
      "lodhaamitind\n",
      "1kiran7\n",
      "angadshingh792\n",
      "hotboxdelhi\n",
      "hardik.kk7\n",
      "dam_goel_\n",
      "ihimanshu_71\n",
      "ketakinadkarni\n",
      "jbbayurveda\n",
      "nikhilsingla01\n",
      "chai_n_smokers\n"
     ]
    }
   ],
   "source": [
    "search = driver.find_element_by_xpath('//div[contains(@class,\"LWmhU\")]/input')      #search for input box\n",
    "search.send_keys('foodtalkindia')                                                   #passing input \n",
    "first_search = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'z556c')))  #first profile\n",
    "time.sleep(2)                                                                       #waiting 2 sec for search\n",
    "first_search.click()                                                                #first search click\n",
    "followers_button = wait.until(EC.presence_of_element_located((By.XPATH,'//a[contains(@class,\"-nal3\")]'))) #follower button search\n",
    "followers_button.click()                                                            #click on followers button\n",
    "flag = False \n",
    "while True:\n",
    "    b = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'isgrP')))         #search for followers\n",
    "    p = driver.find_element_by_class_name('isgrP')\n",
    "    \n",
    "    p.click()                                                        \n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.END)                    #moving end of document \n",
    "    if flag == False:\n",
    "        for i in range(6):\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.PAGE_UP)        #to remove suggestion buttuon \n",
    "            time.sleep(1) \n",
    "        flag = True                                                                #only work for one time\n",
    "    time.sleep(1)\n",
    "    p = driver.find_element_by_class_name('isgrP')                                 #click on page of follower\n",
    "    p.click()\n",
    "    follower_list = driver.find_elements_by_xpath('//div[@class = \"isgrP\"]/ul/div/li')  #follower list\n",
    "    if len(follower_list) >= 500:                                                       #cheking for only 500 follower\n",
    "        break\n",
    "li = []\n",
    "j=0\n",
    "foodtalkindia_follower = []\n",
    "for i in follower_list:                                                                  \n",
    "    data = BeautifulSoup(i.get_attribute('innerHTML'),'html')                         #to get the name of follower             \n",
    "    li.append(data.a['href'].split('/')[1])                                           #adding name in li\n",
    "    foodtalkindia_follower.append(data.a['href'].split('/')[1])\n",
    "    j+=1\n",
    "    if j==500:                                                                        #only top 500 follower\n",
    "        break\n",
    "for i in foodtalkindia_follower:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\public\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\public\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thenidaaa\n",
      "sonu_dillli_kkc\n",
      "merihindikavitaa\n",
      "_ankur._.k_\n",
      "thebegginers\n",
      "garyanlife\n",
      "pinkynarang04\n",
      "shagunrandhawa_\n",
      "_sanju15\n",
      "shreyathakur1990\n",
      "vanessahernandez1969\n",
      "spfl07\n",
      "sambhawigautam\n",
      "laksh1108\n",
      "nomanbinjameel\n",
      "batraanjali\n",
      "morelikediya\n",
      "prince.kathuria.961\n",
      "jaysharmaaaaaa\n",
      "tiyabindal2435\n",
      "april020400\n",
      "sasteyfoodies\n",
      "ramandua76\n",
      "mr.sachu38\n",
      "its_mirandian\n",
      "lil_goofy_360\n",
      "sparshpolychem2006\n",
      "bansal852\n",
      "chatkare_chatpate\n",
      "simpy_yadav_27\n",
      "litt_i_gator\n",
      "the_travel_feet_\n",
      "nitish_1265\n",
      "megha.v_0136\n",
      "aakash_8415\n",
      "vibhuti900shar1\n",
      "aashna7643\n",
      "__.divyaanss\n",
      "hanjra_prabh\n",
      "amansingh4748\n",
      "sanchit.thakur.93\n",
      "surbhijoshix\n",
      "riagupta192003\n",
      "arora228767\n",
      "feastbyavishek\n",
      "149ak.aditya\n",
      "boom_11__\n",
      "barikpradipta\n",
      "pupglover6\n",
      "rathoriahoney\n",
      "innocentthakur2002\n",
      "mikprajapati\n",
      "sumankashyap690\n",
      "sirajulmostofa\n",
      "iamadi01\n",
      "jainishika176\n",
      "therampantbull\n",
      "__thenapqueen__\n",
      "y___a___s___h\n",
      "rapstarb4\n",
      "meghavashisht11\n",
      "neime_wetsah\n",
      "aashna.pradhan20\n",
      "kritikaaajoshiii\n",
      "aryanbharill\n",
      "sanya_jolly\n",
      "shahin.khan8300\n",
      "monikaasharma8\n",
      "komalyadav5555\n",
      "zunaira_khan_8\n",
      "neelmajumder007\n",
      "latikanonu\n",
      "_theartfile_\n",
      "laxmansingh8027\n",
      "sumita_mandal_\n",
      "beegeesgastro\n",
      "shiwali_karn\n",
      "manoj.meghval.94\n",
      "karan1488kumar\n",
      "devil_may_care_kudi\n",
      "_tryingsomething\n",
      "salil.sk.kaushal\n",
      "hitesh_7793\n",
      "ramsingh_rj\n",
      "_haqsefoodie__\n",
      "ajayy_kr\n",
      "_ishita1712\n",
      "mr_harsh_ydv\n",
      "punjabilasagna\n",
      "nirjarasethiaa\n",
      "antarrika\n",
      "bhumikauttam\n",
      "rahmankhan967\n",
      "mdreyaz383\n",
      "anishhaasharma\n",
      "ishashashaaa\n",
      "samiksharana01\n",
      "akanshayadav_x\n",
      "mira_taekook\n",
      "salonigupta29\n",
      "_padi_kathavan_\n",
      "shubhamm420\n",
      "sethi5565\n",
      "ashishsahu9753\n",
      "somya_goyal29\n",
      "kalai_nataraj\n",
      "_sid1101\n",
      "gaurav.lived\n",
      "rhea_oribel\n",
      "radhe_sewag_osian\n",
      "prateekb0304\n",
      "saketpandey420\n",
      "rishav__tc\n",
      "ch_luckyyyy\n",
      "gautamtiwari028\n",
      "_a.rshiii\n",
      "chakravortyneha\n",
      "luckybhati9191\n",
      "bhati_official591\n",
      "aarushi_singh_10\n",
      "kritika_oct96\n",
      "jain_anshikax\n",
      "santasballsackss\n",
      "nyra_paul\n",
      "ak786_anand\n",
      "schauhan_13\n",
      "pcwraps\n",
      "deveshnarula\n",
      "its.aman69\n",
      "hoppibeerapp\n",
      "srinetra_aman\n",
      "words0999\n",
      "kashishs.21\n",
      "__sanyam_jain__\n",
      "meghamanas\n",
      "lavesh_ahir_\n",
      "goldengrapespackaging\n",
      "kundankumarcallmearya\n",
      "sauravthakur66\n",
      "theshubhampandat\n",
      "chirag510jaiswal\n",
      "vidtimes\n",
      "_sakshi.dedha_\n",
      "vidz_pvt12\n",
      "gunjan.srivastava12\n",
      "yash_.bansal_\n",
      "re.em_daisy\n",
      "t_u_s_h1\n",
      "avramrahul\n",
      "aiishasiddiqua11\n",
      "31ayushijain\n",
      "_b.i.g.b.e.a.s.t_\n",
      "irshadkhan4416\n",
      "bhanu_raghav24\n",
      "nilanjan.ghatak\n",
      "i_kratika_24\n",
      "_ashiarora\n",
      "buy_likes_followers__\n",
      "vandapanda14\n",
      "spk1212\n",
      "dimp.1978\n",
      "soni_neha86\n",
      "_seema0208_\n",
      "raakheeanand\n",
      "dheerajdadhich\n",
      "_anuradha_03\n",
      "busy_mommy_meena\n",
      "ayushi1405\n",
      "muskanjain27\n",
      "purvi1104\n",
      "karman_27\n",
      "priya.garg5111\n",
      "paulgal\n",
      "ben_ducci\n",
      "_naina_thakur\n",
      "kartik_lal_chandani\n",
      "prachidua\n",
      "vinayak_aggarwal\n",
      "_karthikanair_\n",
      "kanishq_besoya_dilli001\n",
      "babayegaburgers\n",
      "kolkata_fooditude\n",
      "sarab.sachdeva\n",
      "amaan_6889\n",
      "beast__anime\n",
      "prince__chokan\n",
      "manya_anand_05\n",
      "sumna_r\n",
      "twinkle__sharma\n",
      "thedelhifoodiee\n",
      "travel_with_kg\n",
      "mnv_jotvirk\n",
      "always_listen1\n",
      "blue____valentine\n",
      "joyprettsiingh\n",
      "pushtinigam\n",
      "ravitiwari55\n",
      "farqaleetchishti\n",
      "priya_jaiswal768\n",
      "aarjav.777\n",
      "keshav.abhi1992\n",
      "srkhappy66\n",
      "dhamijavisuals\n",
      "swati_1280\n",
      "advpoojamishra\n",
      "vani__nigam\n",
      "anshul_8_9\n",
      "pashminasilk\n",
      "yachna0327\n",
      "shellyrajput9\n",
      "aryanhunk\n",
      "_a_young_gentleman\n",
      "smoky.tasty\n",
      "parveen.jindal.35\n",
      "rishabh_0997\n",
      "harshitamodi32\n",
      "yuvikathakkar\n",
      "rohi_t7070\n",
      "_rinamehra_\n",
      "mansi__toppo\n",
      "irakhan91\n",
      "dev94168\n",
      "saanchimadan\n",
      "mitushi_srivastava\n",
      "rinklebharti19\n",
      "ekta.luthra\n",
      "jerry_aaru_1623\n",
      "chiieenu\n",
      "gumnam.hi.rehne.do\n",
      "avinashmehrax3\n",
      "udi_0.2\n",
      "divyaarora0505\n",
      "muqeetahmad213\n",
      "rishavtrivedi\n",
      "prateek_sahdev\n",
      "rai_nehahaha\n",
      "isakshi__raturiiii\n",
      "aashnadargan\n",
      "aryamannattri\n",
      "bablutiwariabvp\n",
      "impoooorva\n",
      "mom_of_my_littleworld\n",
      "gautambidhuri\n",
      "imkanak789\n",
      "nihaalsingh99\n",
      "fanoply_2\n",
      "uvjsjuu\n",
      "khurana_vipin28\n",
      "farazuuddin\n",
      "tajnitkaur\n",
      "iamaakashh_\n",
      "swetajdubey19\n",
      "krishgulabbo\n",
      "parveengehl\n",
      "the_nourished_soul_\n",
      "rupa_icecream\n",
      "chauhan_urvashi15\n",
      "shruthi__gs\n",
      "travelerlust\n",
      "sharmasfriend\n",
      "aryan.bansal007\n",
      "sunny.bwoyy\n",
      "_nidhusarwan_\n",
      "tanyagulati0908\n",
      "cynic_underneath\n",
      "aditibaunthiyal\n",
      "hind0500\n",
      "anushrutisachdeva\n",
      "nidhi_chaudhary24\n",
      "mansi_k3\n",
      "kehdunkya\n",
      "_y.a.s.h.i_agarwal\n",
      "sakshipriyamishra\n",
      "iamuroosaahmed\n",
      "raunak5678\n",
      "belovable05\n",
      "goldendragonjaipur\n",
      "vipulkrb\n",
      "nishtha__chauhan\n",
      "nehaj_08\n",
      "goyalmohit0655\n",
      "yayy.kti\n",
      "rahul_baghel00\n",
      "noisy.boie_\n",
      "tejsvi_tannan\n",
      "davysaxena\n",
      "rishigour3\n",
      "govindakunwor\n",
      "mukesh_marwal\n",
      "shalinichaudharyy\n",
      "baiju_p_\n",
      "_radhikkkaaaa_\n",
      "tokas100\n",
      "poetry_unheard29\n",
      "foodie_kookiemochi\n",
      "strolling_himalayas\n",
      "mohitbisht1703\n",
      "eishahmad\n",
      "amit.gupta.5494\n",
      "as221990\n",
      "aaru_rushi\n",
      "iamswt\n",
      "parveen.joy\n",
      "akansha_tyagi21\n",
      "iam__kanika\n",
      "garg.mehak\n",
      "iamfoodiesuraj\n",
      "nannudharmkot\n",
      "i_saakshi\n",
      "sujfresh\n",
      "x_saifi02_x\n",
      "akshatgupta123\n",
      "sorrisopizza.in\n",
      "abstractart2019sq8\n",
      "giggles023\n",
      "bhawna.adlakha1506\n",
      "thenaughtyjokes\n",
      "dhaarna.19\n",
      "nishant_gulati\n",
      "naresh6264kumar\n",
      "shyamsukhasidharth\n",
      "harsh_sngh\n",
      "vatslagupta\n",
      "tarangutan\n",
      "akshay_anantauto\n",
      "rajatnandwani91\n",
      "_radzeeeee\n",
      "jm_2723\n",
      "nupur_sharma5795\n",
      "diumanmahajan\n",
      "ulka.paulka\n",
      "zoyeahh_\n",
      "miracles_divya\n",
      "ashmiie\n",
      "sonamaneja\n",
      "spoon.ofyum\n",
      "bavneetk7\n",
      "cristiano7toyash\n",
      "danish_ara02\n",
      "uncultured_cravings\n",
      "theorientaldwarf\n",
      "iam_brf\n",
      "parmar.maansi\n",
      "akshicat\n",
      "satyajeetmtm\n",
      "himanshu_0201\n",
      "abhijeet_singh4009\n",
      "arabseawadh\n",
      "_mistak3n_\n",
      "neophytes_2k19\n",
      "abhisharma1503\n",
      "therealdira\n",
      "may._.nikita\n",
      "malhotraparkhi\n",
      "nabab.ansari.777\n",
      "rekha.ray.5076\n",
      "saujanya_mahajan\n",
      "bhasinsiddhi\n",
      "shirinhasann\n",
      "choudhry_nouman5031\n",
      "katariyasajjan\n",
      "mehak2512\n",
      "yashhyadavv\n",
      "anvixx\n",
      "tyagidroga\n",
      "foody_gupta_girl\n",
      "feedingfoodiefood\n",
      "ashriamalhotra\n",
      "ch_himanshu_ooo7\n",
      "bhawna.kashyap786.bk\n",
      "__vaniii.poudwal_\n",
      "pranavgupta_27\n",
      "makeupbyravleenkaur\n",
      "models_of_editing\n",
      "_anmoltirkey\n",
      "annusharma8992\n",
      "rastogidevanshi\n",
      "__kartik.tyagi\n",
      "rajputvarsha_\n",
      "rameez_ahmad_sidd\n",
      "kleine_wereld\n",
      "sunny_brahman\n",
      "ashima.09\n",
      "muskaan_sachdeva\n",
      "___vidushiii___\n",
      "riyaarora800\n",
      "tasleem4005\n",
      "makeupbysonal\n",
      "foodilicious_tee\n",
      "kavyyaa__\n",
      "shrutijasrotia\n",
      "vinaya_rana\n",
      "roop30july\n",
      "onlypositive_vibes123\n",
      "kanchi_gautam\n",
      "coolboy0111\n",
      "shiva_abhishek\n",
      "star_eyed_gurl_12\n",
      "saurabh_nogia\n",
      "treating_tastebuds\n",
      "hairstyler_salman\n",
      "iroshnigariya\n",
      "vaishnavidarmwal0201\n",
      "praaacheee\n",
      "thefoodiesiblingz\n",
      "bhavi_rathi30\n",
      "lovequotesreal.in\n",
      "craveable_flavours_food\n",
      "aishna_grover\n",
      "officialdosapoint\n",
      "sanajanachaudhary\n",
      "178xyz\n",
      "akshay_p_deshmukh_\n",
      "dicksonpebam\n",
      "tanishqkhurana20\n",
      "itsaviourj\n",
      "sanjana_bhandari_01\n",
      "puru_mum_mum\n",
      "infinitygamingi\n",
      "ankitasagar177\n",
      "keep_it_wild_\n",
      "heerasingh5656\n",
      "sugargenius.28\n",
      "gaurav.gumber167\n",
      "life14311love\n",
      "somyaasthana\n",
      "sunny5347kumar\n",
      "harsuooo\n",
      "deoresweety\n",
      "agrawalindu\n",
      "smileyyy626\n",
      "prarthana.barua\n",
      "deep_parcha\n",
      "namrta_827\n",
      "kanika918\n",
      "mehika_arora\n",
      "rahul.qwerty\n",
      "jaiswal0103\n",
      "adhyyan_prasher\n",
      "akash.bamal\n",
      "tayyab_siddiqui6\n",
      "swatibahetii\n",
      "achintyasinghmusic\n",
      "harshita.varshney\n",
      "shereenbajaj\n",
      "shivanshunegi\n",
      "gujjar_bhupendar\n",
      "r_a_c_h19\n",
      "_shivangi_solanki_\n",
      "sardaarjifoodie\n",
      "kasolmusicfestivalofficial\n",
      "venkateshgupta5\n",
      "daksh1274\n",
      "_prachibhardwaj_\n",
      "raghavdeepak08\n",
      "awl_aail_have\n",
      "ashna.anand29\n",
      "vanshika_1403\n",
      "beerbatteredego\n",
      "dakshinadandriyal\n",
      "maan.savinder\n",
      "akriti29rathore\n",
      "aish_singh_163\n",
      "sheenjita\n",
      "jyotsna_ghai10\n",
      "anubhavsharma_tsboh\n",
      "vipinthecoach\n",
      "i.am.abhishek.verma\n",
      "payaldhingra5\n",
      "srdude1234\n",
      "chawlarohit_1115\n",
      "sachinkaghra\n",
      "a80j80\n",
      "mukherjeesuvam\n",
      "sabal_1_gupta\n",
      "jitendra_patole_\n",
      "niti_sahni\n",
      "arulbhasin14\n",
      "chadhaaditi_04\n",
      "jaanvibajaj12\n",
      "tastosaurous\n",
      "mehakgupta5\n",
      "shivania185\n",
      "gupta_deeksha\n",
      "simrit08\n",
      "ankitmittal1991\n",
      "harshthakur_hr\n",
      "itsmekratika31\n",
      "prerna_malhotra10\n",
      "okplsshutup\n",
      "kanikabansal167\n",
      "reminiscing_new_moon\n",
      "ginalinettii\n",
      "florence_f_l_o_w\n",
      "esha.2831\n",
      "bachpansebhookhe\n",
      "meet_the_meet_987\n",
      "foodie_punjaban7\n",
      "anjali__maurya\n",
      "i_m_zuher\n"
     ]
    }
   ],
   "source": [
    "search = driver.find_element_by_xpath('//div[contains(@class,\"LWmhU\")]/input')              #input box\n",
    "search.send_keys('sodelhi')\n",
    "first_search = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'z556c')))          #first profile\n",
    "first_search.click()\n",
    "time.sleep(2)\n",
    "followers_button = wait.until(EC.presence_of_element_located((By.XPATH,'//a[contains(@class,\"-nal3\")]')))   #follower button\n",
    "followers_button.click()\n",
    "flag = False\n",
    "while True:\n",
    "    p = driver.find_element_by_class_name('isgrP')                                           #follower page\n",
    "    p.click()\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.END)                              #moving to end of page\n",
    "    if flag == False:\n",
    "        for i in range(6):\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.PAGE_UP)                  #make 6 time page up to remove first suggestion\n",
    "            time.sleep(1)\n",
    "        flag = True\n",
    "    time.sleep(1)\n",
    "    p = driver.find_element_by_class_name('isgrP')                                          #follwer page\n",
    "    p.click()\n",
    "    follower_list = driver.find_elements_by_xpath('//div[@class = \"isgrP\"]/ul/div/li')      #folllower list\n",
    "    if len(follower_list) >= 500:\n",
    "        break\n",
    "li = []\n",
    "j=0\n",
    "names = []\n",
    "for i in follower_list:\n",
    "    data = BeautifulSoup(i.get_attribute('innerHTML'),'html')                            #alreday explained \n",
    "    li.append(data.a['href'].split('/')[1])\n",
    "    names.append(data.a['href'].split('/')[1])\n",
    "    j+=1\n",
    "    if j==500:\n",
    "        break\n",
    "for i in names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6.2) Now print all the followers of “foodtalkindia” that you are following but those who don’t follow you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\public\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\public\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ver.francis\n",
      "muzaynachohdry\n",
      "anchalo\n",
      "luxurious_tarunwadhwani\n",
      "krish__2019\n",
      "getyourmenu_\n",
      "prarthana.gehlot\n",
      "atul_dixit05\n",
      "falam_thefruitcafe\n",
      "azeem.suleman.7\n",
      "mehulharia\n",
      "ryubarexclusive\n",
      "ayah_romo\n",
      "vrushali.rao.98\n",
      "bitescorporate\n"
     ]
    }
   ],
   "source": [
    "search = driver.find_element_by_xpath('//div[contains(@class,\"LWmhU\")]/input')\n",
    "search.send_keys('foodtalkindia')\n",
    "first_search = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'z556c')))\n",
    "first_search.click()\n",
    "time.sleep(2)\n",
    "followers_button = wait.until(EC.presence_of_element_located((By.XPATH,'//a[contains(@class,\"-nal3\")]')))\n",
    "followers_button.click()\n",
    "flag = False\n",
    "while True:\n",
    "    p = driver.find_element_by_class_name('isgrP')\n",
    "    p.click()\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "    if flag == False:\n",
    "        for i in range(6):\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.PAGE_UP)\n",
    "            time.sleep(1)\n",
    "        flag = True\n",
    "    time.sleep(1)\n",
    "    p = driver.find_element_by_class_name('isgrP')\n",
    "    p.click()\n",
    "    follower_list = driver.find_elements_by_xpath('//div[@class = \"isgrP\"]/ul/div/li')\n",
    "    if len(follower_list) >= 500:\n",
    "        break \n",
    "li = []\n",
    "j=0\n",
    "names = []\n",
    "for i in follower_list:\n",
    "    data = BeautifulSoup(i.get_attribute('innerHTML'),'html')\n",
    "    li.append(data.a['href'].split('/')[1])\n",
    "    names.append(data.a['href'].split('/')[1])\n",
    "    j+=1\n",
    "    if j==500:\n",
    "        break\n",
    "i=0;\n",
    "for p in follower_list:\n",
    "    data = BeautifulSoup(p.get_attribute('innerHTML'),'html')\n",
    "    if data.button.text==\"Following\":                                #only printing that follwoing that don't follow u back \n",
    "        print(names[i]) \n",
    "    i=i+1\n",
    "count=0      #for story check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7)Check the story of ‘coding.ninjas’. Consider the following Scenarios and print error messages accordingly -\n",
    "### (7.1)If You have already seen the story.\n",
    "### (7.2)Or The user has no story.\n",
    "### (7.3)Or View the story if not yet seen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not checking the story yet and now i am going to check it\n",
      "Already seen the Story.\n"
     ]
    }
   ],
   "source": [
    "search = driver.find_element_by_xpath('//div[contains(@class,\"LWmhU\")]/input')      #input box  \n",
    "search.send_keys('coding.ninjas')\n",
    "first_search = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'z556c')))  #fisrt profile\n",
    "first_search.click()\n",
    "print('Not checking the story yet and now i am going to check it')\n",
    "time.sleep(2)\n",
    "try:\n",
    "    a = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'IalUJ'))) #if user has story then it will give TimeoutException\n",
    "    if a.click()==None:\n",
    "        print(\"User don't has Story.\")\n",
    "        \n",
    "except TimeoutException:\n",
    "    time.sleep(2)\n",
    "    if count==1:\n",
    "        print(\"Already seen the Story.\")\n",
    "    else:\n",
    "        a = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'_2dbep')))              #for story check button\n",
    "        a.click()\n",
    "        print(\"Seen the story\")\n",
    "        count+=1\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
